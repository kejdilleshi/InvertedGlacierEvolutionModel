{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration as a dictionary\n",
    "config = {\n",
    "    \"nb_layers\": 4,               # Number of convolutional layers\n",
    "    \"nb_out_filter\": 32,           # Number of output filters for Conv2D\n",
    "    \"conv_ker_size\": 3,            # Convolution kernel size\n",
    "    \"activation\": \"relu\",          # Activation function: \"relu\" or \"lrelu\"\n",
    "    \"dropout_rate\": 0.1,           # Dropout rate\n",
    "    \"regularization\": 0.0001       # L2 regularization\n",
    "}\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_outputs, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = nb_inputs\n",
    "        for _ in range(config['nb_layers']):\n",
    "            layers.append(nn.Conv2d(in_channels, config['nb_out_filter'], kernel_size=config['conv_ker_size'], padding='same'))\n",
    "            layers.append(nn.ReLU() if config['activation'] == \"relu\" else nn.LeakyReLU(0.01))\n",
    "            layers.append(nn.Dropout(config['dropout_rate']))\n",
    "            in_channels = config['nb_out_filter']\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Conv2d(in_channels, nb_outputs, kernel_size=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "# Define the number of input channels (thk, slopsurfx, slopsurfy) and output channels (ubar, vbar)\n",
    "nb_inputs = 3  # thk, slopsurfx, slopsurfy\n",
    "nb_outputs = 2  # ubar, vbar\n",
    "\n",
    "# Instantiate the CNN model\n",
    "\n",
    "model = CNN(nb_inputs, nb_outputs, config).to(device)\n",
    "model.load_state_dict(torch.load('/home/klleshi/IGEM/InvertedGlacierEvolutionModel/glacier_flow_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch version of compute_divflux\n",
    "def compute_divflux(u, v, h, dx, dy):\n",
    "    \"\"\"\n",
    "    Upwind computation of the divergence of the flux: d(u h)/dx + d(v h)/dy.\n",
    "    \"\"\"\n",
    "    # Compute u and v on the staggered grid\n",
    "    u = torch.cat([u[:, :1], 0.5 * (u[:, :-1] + u[:, 1:]), u[:, -1:]], dim=1)  # shape (ny, nx+1)\n",
    "    v = torch.cat([v[:1, :], 0.5 * (v[:-1, :] + v[1:, :]), v[-1:, :]], dim=0)  # shape (ny+1, nx)\n",
    "\n",
    "    # Extend h with constant value at the domain boundaries\n",
    "    Hx = torch.nn.functional.pad(h, (1, 1, 0, 0), mode=\"constant\")  # shape (ny, nx+2)\n",
    "    Hy = torch.nn.functional.pad(h, (0, 0, 1, 1), mode=\"constant\")  # shape (ny+2, nx)\n",
    "\n",
    "    # Compute fluxes by selecting the upwind quantities\n",
    "    Qx = u * torch.where(u > 0, Hx[:, :-1], Hx[:, 1:])  # shape (ny, nx+1)\n",
    "    Qy = v * torch.where(v > 0, Hy[:-1, :], Hy[1:, :])  # shape (ny+1, nx)\n",
    "\n",
    "    # Compute the divergence, final shape is (ny, nx)\n",
    "    divflux = (Qx[:, 1:] - Qx[:, :-1]) / dx + (Qy[1:, :] - Qy[:-1, :]) / dy\n",
    "    return divflux\n",
    "\n",
    "\n",
    "# Define PyTorch version of compute_gradient\n",
    "def compute_gradient(s, dx, dy):\n",
    "    \"\"\"\n",
    "    Compute spatial 2D gradient of a given field.\n",
    "    \"\"\"\n",
    "    EX = torch.cat([1.5 * s[:, :1] - 0.5 * s[:, 1:2], 0.5 * (s[:, :-1] + s[:, 1:]), 1.5 * s[:, -1:] - 0.5 * s[:, -2:-1]], dim=1)\n",
    "    diffx = (EX[:, 1:] - EX[:, :-1]) / dx\n",
    "\n",
    "    EY = torch.cat([1.5 * s[:1, :] - 0.5 * s[1:2, :], 0.5 * (s[:-1, :] + s[1:, :]), 1.5 * s[-1:, :] - 0.5 * s[-2:-1, :]], dim=0)\n",
    "    diffy = (EY[1:, :] - EY[:-1, :]) / dy\n",
    "\n",
    "    return diffx, diffy\n",
    "\n",
    "\n",
    "# Apply boundary condition in PyTorch\n",
    "def apply_boundary_condition(H_ice, boundary_width=5):\n",
    "    \"\"\"\n",
    "    Apply boundary condition to the ice thickness field `H_ice`.\n",
    "    \"\"\"\n",
    "    ny, nx = H_ice.shape\n",
    "\n",
    "    # Create linear ramps\n",
    "    ramp = torch.linspace(1, 0, boundary_width, device=H_ice.device)\n",
    "\n",
    "    # Apply boundary condition to the left and right boundaries\n",
    "    H_ice[:, :boundary_width] *= ramp.flip(0)  # Left\n",
    "    H_ice[:, -boundary_width:] *= ramp  # Right\n",
    "\n",
    "    # Apply boundary condition to the top and bottom boundaries\n",
    "    H_ice[:boundary_width, :] *= ramp.flip(0).unsqueeze(1)  # Top\n",
    "    H_ice[-boundary_width:, :] *= ramp.unsqueeze(1)  # Bottom\n",
    "\n",
    "    return H_ice\n",
    "def print_peak_gpu_memory():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
    "    print(f\"Peak GPU memory used: {peak_memory:.2f} MB.\")\n",
    "    torch.cuda.reset_peak_memory_stats()  # Reset after printing\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()  # Initialize NVML\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)  # Assuming we're using GPU 0\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)  # Get memory info\n",
    "    print(f\"GPU memory occupied: {info.used // 1024**2} MB.\")\n",
    "\n",
    "def visualize(Z_surf,time,H_ice,Lx,Ly):\n",
    "        clear_output(wait=True)  # Clear the previous output in the notebook\n",
    "        plt.figure(2, figsize=(11, 4), dpi=200)\n",
    "        # First subplot: Ice surface\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(Z_surf.cpu().numpy(), extent=[0, Lx/1000, 0, Ly/1000], cmap='terrain', origin='lower')\n",
    "        plt.colorbar(label='Elevation (m)')\n",
    "        plt.title('Ice Surface at ' + str(int(time)) + ' y')\n",
    "        plt.xlabel('Distance, km')\n",
    "        plt.ylabel('Distance, km')\n",
    "        # Second subplot: Ice thickness\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.where(H_ice.cpu().numpy() > 0, H_ice.cpu().numpy(), np.nan), extent=[0, Lx/1000, 0, Ly/1000], cmap='jet', origin='lower')\n",
    "        plt.colorbar(label='Ice Thickness (m)')\n",
    "        plt.title('Ice Thickness at ' + str(int(time)) + ' y')\n",
    "        plt.xlabel('Distance, km')\n",
    "        plt.ylabel('Distance, km')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Prepare hooks for tensors \n",
    "def print_hook_b(grad):\n",
    "    print(\"\\n db/dELA:\",torch.mean(grad))\n",
    "    return grad\n",
    "def print_hook_h(grad):\n",
    "    print(\"\\n dH/db:\",torch.mean(grad))\n",
    "    return grad\n",
    "def sqrt_hook(grad):\n",
    "    if torch.abs(torch.mean(grad))>1:\n",
    "        print(grad**0.5)\n",
    "        return grad**0.5\n",
    "def reduce_hook(grad):\n",
    "    return grad*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "ttot = 900  # Time limit (yr)\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = 2700  # Elevation of equilibrium line altitude (m)\n",
    "\n",
    "# Load the topography data\n",
    "nc_file = netCDF4.Dataset('bedrock.nc')\n",
    "Z_topo = torch.tensor(nc_file.variables['topg'][:], device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Scaling factors (assuming they were precomputed)\n",
    "scaling_factors = {\n",
    "    \"thk\": 780.0,  # Example values; replace with actual scaling factors\n",
    "    \"slopsurfx\": 1.8200000524520874,\n",
    "    \"slopsurfy\": 1.6096999645233154,\n",
    "    \"ubar\": 346.2121276855469,\n",
    "    \"vbar\": 173.1743927001953\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors,Lx = 49700,Ly = 32300,dx=100,dy=100,cfl = 0.20,dtmax = 1,device=device):\n",
    "\n",
    "    # Physical parameters (unchanged)\n",
    "\n",
    "    nx = int(Lx / dx)\n",
    "    ny = int(Ly / dy)\n",
    "\n",
    "    # Initialize ice thickness and surface\n",
    "    H_ice = torch.zeros((ny, nx), device=device, dtype=torch.float32)\n",
    "    Z_surf = Z_topo + H_ice\n",
    "    \n",
    "    # Compute initial gradients of surface elevation (slopes)\n",
    "    slopsurfx, slopsurfy = compute_gradient(Z_surf, dx, dy)\n",
    "    \n",
    "    # Initialize time and iteration counter\n",
    "    time = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "    dt = torch.tensor(dtmax, device=device, dtype=torch.float32)\n",
    "    it = 0\n",
    "\n",
    "\n",
    "    # Main simulation loop\n",
    "    while time < ttot:\n",
    "        time += dt\n",
    "        it += 1\n",
    "\n",
    "        # Scale the inputs with stored scaling factors\n",
    "        H_ice_scaled = H_ice / scaling_factors[\"thk\"]\n",
    "        slopsurfx_scaled = slopsurfx / scaling_factors[\"slopsurfx\"]\n",
    "        slopsurfy_scaled = slopsurfy / scaling_factors[\"slopsurfy\"]\n",
    "\n",
    "        # Combine scaled inputs and add batch dimension\n",
    "        input_data_scaled = torch.stack([H_ice_scaled, slopsurfx_scaled, slopsurfy_scaled], dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Use the trained model to predict ubar (x-velocity) and vbar (y-velocity)\n",
    "        with torch.no_grad():\n",
    "            ubar_vbar_pred = model(input_data_scaled.permute(0, 3, 1, 2))  # Change to (batch, channels, height, width)\n",
    "            ubar = ubar_vbar_pred[0, 0, :, :] * scaling_factors[\"ubar\"]  # x-component of velocity\n",
    "            vbar = ubar_vbar_pred[0, 1, :, :] * scaling_factors[\"vbar\"]  # y-component of velocity\n",
    "\n",
    "        # Compute maximum velocity for CFL condition\n",
    "        vel_max = max(ubar.abs().max().item(), vbar.abs().max().item())\n",
    "\n",
    "        # Compute time step (CFL condition)\n",
    "        dt = min(cfl * dx / vel_max, dtmax)\n",
    "\n",
    "        # Update rule (diffusion): Compute the change in thickness (dH/dt)\n",
    "        dHdt = -compute_divflux(ubar, vbar, H_ice, dx, dy)\n",
    "\n",
    "        # Update ice thickness and ensure no negative values\n",
    "        H_ice += dt * dHdt\n",
    "        H_ice = torch.clamp(H_ice, min=0)\n",
    "\n",
    "        # Define the SMB (Surface Mass Balance)\n",
    "        b = torch.minimum(grad_b * (Z_surf - Z_ELA), torch.tensor(b_max, device=device))\n",
    "\n",
    "        # Update rule (mass balance)\n",
    "        H_ice += dt * b\n",
    "\n",
    "        # Apply the boundary condition before the next iteration\n",
    "        H_ice = apply_boundary_condition(H_ice)\n",
    "\n",
    "        # Update surface topography\n",
    "        Z_surf = Z_topo + H_ice\n",
    "\n",
    "        # Compute gradients of surface elevation (slopes)\n",
    "        slopsurfx, slopsurfy = compute_gradient(Z_surf, dx, dy)  \n",
    "        # Display\n",
    "        if it % 50 == 0:\n",
    "            visualize(Z_surf,time,H_ice,Lx,Ly)\n",
    "        \n",
    "    return H_ice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the observations.\n",
    "h=forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors)\n",
    "# torch.save(h,'Obs_2D.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()  # Print memory after the loop\n",
    "print_peak_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "# Initial guesses for inversion problem\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = torch.tensor(3000.0, requires_grad=True, device=device)\n",
    "\n",
    "# Observed glacier thickness (assumed already loaded as observed_thk tensor)\n",
    "observed_thk = torch.load('Obs_2D.pt', weights_only=True).to(device) # Ensure it's on the right device\n",
    "\n",
    "# Define initial and final learning rates\n",
    "initial_lr = 7\n",
    "final_lr = 3\n",
    "n_iterations = 80\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam([Z_ELA], lr=initial_lr)\n",
    "\n",
    "# Initialize lists to track loss components\n",
    "total_loss_history = []\n",
    "data_fidelity_history = []\n",
    "regularization_history = []\n",
    "total_gradients_history=[]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Update the learning rate\n",
    "    lr = initial_lr - (i / (n_iterations - 1)) * (initial_lr - final_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    \n",
    "    # Perform forward simulation\n",
    "    H_simulated = forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors)\n",
    "\n",
    "    # Compute data missfit\n",
    "    loss = torch.mean((H_simulated - observed_thk)**2)\n",
    "     \n",
    "    # Backpropagate loss and update parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients to the range [-1, 1]\n",
    "    # torch.nn.utils.clip_grad_value_(z_ELA, clip_value=10.0)\n",
    "    # Store the gradients and ela \n",
    "    total_gradients_history.append(Z_ELA.grad.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store loss components for plotting later\n",
    "    total_loss_history.append(loss.item())\n",
    "        # Print loss, gradients and current parameters every 50 iterations \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}, Loss: {loss:.3f} \")\n",
    "        print(f\"Gradient of ELA : {Z_ELA.grad.item()} ELA is {Z_ELA} m\")\n",
    "\n",
    "print_gpu_utilization()  # Print memory after the loop\n",
    "print_peak_gpu_memory()  # Print the peak memory   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Initial guesses for inversion problem\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = torch.tensor(3000.0, requires_grad=True, device=device)\n",
    "\n",
    "# Observed glacier thickness (assumed already loaded as observed_thk tensor)\n",
    "observed_thk = torch.load('Obs_2D.pt', weights_only=True).to(device)  # Ensure it's on the right device\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 80\n",
    "\n",
    "# Optimizer setup: Using L-BFGS\n",
    "optimizer = torch.optim.LBFGS([Z_ELA], lr=1, max_iter=20, history_size=10, line_search_fn='strong_wolfe')\n",
    "\n",
    "# Initialize lists to track loss components\n",
    "total_loss_history = []\n",
    "total_gradients_history = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Define closure function required by LBFGS optimizer\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        # Perform forward simulation\n",
    "        H_simulated = forward_simulation(Z_ELA, grad_b, b_max, Z_topo, ttot, scaling_factors)\n",
    "        # Compute data misfit\n",
    "        loss = torch.mean((H_simulated - observed_thk) ** 2)\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    print_gpu_utilization()  # Print memory after the loop\n",
    "    print_peak_gpu_memory()  # Print the peak memory   \n",
    "\n",
    "    # Perform optimization step\n",
    "    loss = optimizer.step(closure)\n",
    "\n",
    "    # Store the gradients and ELA\n",
    "    total_gradients_history.append(Z_ELA.grad.item())\n",
    "\n",
    "    # Store loss components for plotting later\n",
    "    total_loss_history.append(loss.item())\n",
    "\n",
    "    # Print loss, gradients, and current parameters every 20 iterations\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}, Loss: {loss.item():.3f}\")\n",
    "        print(f\"Gradient of ELA: {Z_ELA.grad.item()}, ELA is {Z_ELA.item()} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do :\n",
    "\n",
    "- Convert all variables to fp16\n",
    "- Check if Gradient accumulation is possible\n",
    "- Read the documentation on memory handling (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# plt.plot(ELA_evolution,label=\"evolution of ELA\",color='b')\n",
    "plt.plot(total_gradients_history, label='Evolution of gradients')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the loss function components\n",
    "def plot_loss_components(total_loss_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the total loss, data fidelity, and regularization term\n",
    "    plt.plot(total_loss_history, label='Loss', color='b', linewidth=2)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Loss Function Components Over Iterations')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the optimization loop\n",
    "plot_loss_components(total_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IGEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
