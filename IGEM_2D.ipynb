{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klleshi/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m nb_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# ubar, vbar\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Instantiate the CNN model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/klleshi/IGEM/InvertedGlacierEvolutionModel/glacier_flow_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration as a dictionary\n",
    "config = {\n",
    "    \"nb_layers\": 4,               # Number of convolutional layers\n",
    "    \"nb_out_filter\": 32,           # Number of output filters for Conv2D\n",
    "    \"conv_ker_size\": 3,            # Convolution kernel size\n",
    "    \"activation\": \"relu\",          # Activation function: \"relu\" or \"lrelu\"\n",
    "    \"dropout_rate\": 0.1,           # Dropout rate\n",
    "    \"regularization\": 0.0001       # L2 regularization\n",
    "}\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_outputs, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = nb_inputs\n",
    "        for _ in range(config['nb_layers']):\n",
    "            layers.append(nn.Conv2d(in_channels, config['nb_out_filter'], kernel_size=config['conv_ker_size'], padding='same'))\n",
    "            layers.append(nn.ReLU() if config['activation'] == \"relu\" else nn.LeakyReLU(0.01))\n",
    "            layers.append(nn.Dropout(config['dropout_rate']))\n",
    "            in_channels = config['nb_out_filter']\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Conv2d(in_channels, nb_outputs, kernel_size=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "# Define the number of input channels (thk, slopsurfx, slopsurfy) and output channels (ubar, vbar)\n",
    "nb_inputs = 3  # thk, slopsurfx, slopsurfy\n",
    "nb_outputs = 2  # ubar, vbar\n",
    "\n",
    "# Instantiate the CNN model\n",
    "\n",
    "model = CNN(nb_inputs, nb_outputs, config).to(device)\n",
    "model.load_state_dict(torch.load('/home/klleshi/IGEM/InvertedGlacierEvolutionModel/glacier_flow_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch version of compute_divflux\n",
    "def compute_divflux(u, v, h, dx, dy):\n",
    "    \"\"\"\n",
    "    Upwind computation of the divergence of the flux: d(u h)/dx + d(v h)/dy.\n",
    "    \"\"\"\n",
    "    # Compute u and v on the staggered grid\n",
    "    u = torch.cat([u[:, :1], 0.5 * (u[:, :-1] + u[:, 1:]), u[:, -1:]], dim=1)  # shape (ny, nx+1)\n",
    "    v = torch.cat([v[:1, :], 0.5 * (v[:-1, :] + v[1:, :]), v[-1:, :]], dim=0)  # shape (ny+1, nx)\n",
    "\n",
    "    # Extend h with constant value at the domain boundaries\n",
    "    Hx = torch.nn.functional.pad(h, (1, 1, 0, 0), mode=\"constant\")  # shape (ny, nx+2)\n",
    "    Hy = torch.nn.functional.pad(h, (0, 0, 1, 1), mode=\"constant\")  # shape (ny+2, nx)\n",
    "\n",
    "    # Compute fluxes by selecting the upwind quantities\n",
    "    Qx = u * torch.where(u > 0, Hx[:, :-1], Hx[:, 1:])  # shape (ny, nx+1)\n",
    "    Qy = v * torch.where(v > 0, Hy[:-1, :], Hy[1:, :])  # shape (ny+1, nx)\n",
    "\n",
    "    # Compute the divergence, final shape is (ny, nx)\n",
    "    divflux = (Qx[:, 1:] - Qx[:, :-1]) / dx + (Qy[1:, :] - Qy[:-1, :]) / dy\n",
    "    return divflux\n",
    "\n",
    "\n",
    "# Define PyTorch version of compute_gradient\n",
    "def compute_gradient(s, dx, dy):\n",
    "    \"\"\"\n",
    "    Compute spatial 2D gradient of a given field.\n",
    "    \"\"\"\n",
    "    EX = torch.cat([1.5 * s[:, :1] - 0.5 * s[:, 1:2], 0.5 * (s[:, :-1] + s[:, 1:]), 1.5 * s[:, -1:] - 0.5 * s[:, -2:-1]], dim=1)\n",
    "    diffx = (EX[:, 1:] - EX[:, :-1]) / dx\n",
    "\n",
    "    EY = torch.cat([1.5 * s[:1, :] - 0.5 * s[1:2, :], 0.5 * (s[:-1, :] + s[1:, :]), 1.5 * s[-1:, :] - 0.5 * s[-2:-1, :]], dim=0)\n",
    "    diffy = (EY[1:, :] - EY[:-1, :]) / dy\n",
    "\n",
    "    return diffx, diffy\n",
    "\n",
    "\n",
    "# Apply boundary condition in PyTorch\n",
    "def apply_boundary_condition(H_ice, boundary_width=5):\n",
    "    \"\"\"\n",
    "    Apply boundary condition to the ice thickness field `H_ice`.\n",
    "    \"\"\"\n",
    "    ny, nx = H_ice.shape\n",
    "\n",
    "    # Create linear ramps\n",
    "    ramp = torch.linspace(1, 0, boundary_width, device=H_ice.device)\n",
    "\n",
    "    # Apply boundary condition to the left and right boundaries\n",
    "    H_ice[:, :boundary_width] *= ramp.flip(0)  # Left\n",
    "    H_ice[:, -boundary_width:] *= ramp  # Right\n",
    "\n",
    "    # Apply boundary condition to the top and bottom boundaries\n",
    "    H_ice[:boundary_width, :] *= ramp.flip(0).unsqueeze(1)  # Top\n",
    "    H_ice[-boundary_width:, :] *= ramp.unsqueeze(1)  # Bottom\n",
    "\n",
    "    return H_ice\n",
    "def print_peak_gpu_memory():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
    "    print(f\"Peak GPU memory used: {peak_memory:.2f} MB.\")\n",
    "    torch.cuda.reset_peak_memory_stats()  # Reset after printing\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()  # Initialize NVML\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)  # Assuming we're using GPU 0\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)  # Get memory info\n",
    "    print(f\"GPU memory occupied: {info.used // 1024**2} MB.\")\n",
    "\n",
    "def visualize(Z_surf,time,H_ice,Lx,Ly):\n",
    "        clear_output(wait=True)  # Clear the previous output in the notebook\n",
    "        plt.figure(2, figsize=(11, 4), dpi=200)\n",
    "        # First subplot: Ice surface\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(Z_surf.cpu().numpy(), extent=[0, Lx/1000, 0, Ly/1000], cmap='terrain', origin='lower')\n",
    "        plt.colorbar(label='Elevation (m)')\n",
    "        plt.title('Ice Surface at ' + str(int(time)) + ' y')\n",
    "        plt.xlabel('Distance, km')\n",
    "        plt.ylabel('Distance, km')\n",
    "        # Second subplot: Ice thickness\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.where(H_ice.cpu().numpy() > 0, H_ice.cpu().numpy(), np.nan), extent=[0, Lx/1000, 0, Ly/1000], cmap='jet', origin='lower')\n",
    "        plt.colorbar(label='Ice Thickness (m)')\n",
    "        plt.title('Ice Thickness at ' + str(int(time)) + ' y')\n",
    "        plt.xlabel('Distance, km')\n",
    "        plt.ylabel('Distance, km')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Prepare hooks for tensors \n",
    "def print_hook_b(grad):\n",
    "    print(\"\\n db/dELA:\",torch.mean(grad))\n",
    "    return grad\n",
    "def print_hook_h(grad):\n",
    "    print(\"\\n dH/db:\",torch.mean(grad))\n",
    "    return grad\n",
    "def sqrt_hook(grad):\n",
    "    if torch.abs(torch.mean(grad))>1:\n",
    "        print(grad**0.5)\n",
    "        return grad**0.5\n",
    "def reduce_hook(grad):\n",
    "    return grad*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the topography data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m nc_file \u001b[38;5;241m=\u001b[39m netCDF4\u001b[38;5;241m.\u001b[39mDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbedrock.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m Z_topo \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnc_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Scaling factors (assuming they were precomputed)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaling_factors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m780.0\u001b[39m,  \u001b[38;5;66;03m# Example values; replace with actual scaling factors\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslopsurfx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.8200000524520874\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvbar\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m173.1743927001953\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Physical parameters\n",
    "ttot = 900  # Time limit (yr)\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = 3200  # Elevation of equilibrium line altitude (m)\n",
    "\n",
    "# Load the topography data\n",
    "nc_file = netCDF4.Dataset('bedrock.nc')\n",
    "Z_topo = torch.tensor(nc_file.variables['topg'][:], device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Scaling factors (assuming they were precomputed)\n",
    "scaling_factors = {\n",
    "    \"thk\": 780.0,  # Example values; replace with actual scaling factors\n",
    "    \"slopsurfx\": 1.8200000524520874,\n",
    "    \"slopsurfy\": 1.6096999645233154,\n",
    "    \"ubar\": 346.2121276855469,\n",
    "    \"vbar\": 173.1743927001953\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors,Lx = 49700,Ly = 32300,dx=100,dy=100,cfl = 0.20,dtmax = 1,device=device):\n",
    "\n",
    "    # Physical parameters (unchanged)\n",
    "\n",
    "    nx = int(Lx / dx)\n",
    "    ny = int(Ly / dy)\n",
    "\n",
    "    # Initialize ice thickness and surface\n",
    "    H_ice = torch.zeros((ny, nx), device=device, dtype=torch.float32)\n",
    "    Z_surf = Z_topo + H_ice\n",
    "    \n",
    "    # Compute initial gradients of surface elevation (slopes)\n",
    "    slopsurfx, slopsurfy = compute_gradient(Z_surf, dx, dy)\n",
    "    \n",
    "    # Initialize time and iteration counter\n",
    "    time = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "    dt = torch.tensor(dtmax, device=device, dtype=torch.float32)\n",
    "    it = 0\n",
    "\n",
    "\n",
    "    # Main simulation loop\n",
    "    while time < ttot:\n",
    "        time += dt\n",
    "        it += 1\n",
    "\n",
    "        # Scale the inputs with stored scaling factors\n",
    "        H_ice_scaled = H_ice / scaling_factors[\"thk\"]\n",
    "        slopsurfx_scaled = slopsurfx / scaling_factors[\"slopsurfx\"]\n",
    "        slopsurfy_scaled = slopsurfy / scaling_factors[\"slopsurfy\"]\n",
    "\n",
    "        # Combine scaled inputs and add batch dimension\n",
    "        input_data_scaled = torch.stack([H_ice_scaled, slopsurfx_scaled, slopsurfy_scaled], dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Use the trained model to predict ubar (x-velocity) and vbar (y-velocity)\n",
    "        with torch.no_grad():\n",
    "            ubar_vbar_pred = model(input_data_scaled.permute(0, 3, 1, 2))  # Change to (batch, channels, height, width)\n",
    "            ubar = ubar_vbar_pred[0, 0, :, :] * scaling_factors[\"ubar\"]  # x-component of velocity\n",
    "            vbar = ubar_vbar_pred[0, 1, :, :] * scaling_factors[\"vbar\"]  # y-component of velocity\n",
    "\n",
    "        # Compute maximum velocity for CFL condition\n",
    "        vel_max = max(ubar.abs().max().item(), vbar.abs().max().item())\n",
    "\n",
    "        # Compute time step (CFL condition)\n",
    "        dt = min(cfl * dx / vel_max, dtmax)\n",
    "\n",
    "        # Update rule (diffusion): Compute the change in thickness (dH/dt)\n",
    "        dHdt = -compute_divflux(ubar, vbar, H_ice, dx, dy)\n",
    "\n",
    "        # Update ice thickness and ensure no negative values\n",
    "        H_ice += dt * dHdt\n",
    "        H_ice = torch.clamp(H_ice, min=0)\n",
    "\n",
    "        # Define the SMB (Surface Mass Balance)\n",
    "        b = torch.minimum(grad_b * (Z_surf - Z_ELA), torch.tensor(b_max, device=device))\n",
    "\n",
    "        # Update rule (mass balance)\n",
    "        H_ice += dt * b\n",
    "\n",
    "        # Apply the boundary condition before the next iteration\n",
    "        H_ice = apply_boundary_condition(H_ice)\n",
    "\n",
    "        # Update surface topography\n",
    "        Z_surf = Z_topo + H_ice\n",
    "\n",
    "        # Compute gradients of surface elevation (slopes)\n",
    "        slopsurfx, slopsurfy = compute_gradient(Z_surf, dx, dy)  \n",
    "        # Display\n",
    "        # if it % 50 == 0:\n",
    "        #     visualize(Z_surf,time,H_ice,Lx,Ly)\n",
    "        \n",
    "    return H_ice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the observations.\n",
    "# h=forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors)\n",
    "# torch.save(h,'Obs_2D.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 254 MB.\n",
      "Peak GPU memory used: 0.00 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()  # Print memory after the loop\n",
    "print_peak_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "# Initial guesses for inversion problem\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = torch.tensor(3000.0, requires_grad=True, device=device)\n",
    "\n",
    "# Observed glacier thickness (assumed already loaded as observed_thk tensor)\n",
    "observed_thk = torch.load('Obs_2D.pt', weights_only=True).to(device) # Ensure it's on the right device\n",
    "\n",
    "# Define initial and final learning rates\n",
    "initial_lr = 7\n",
    "final_lr = 3\n",
    "n_iterations = 80\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam([Z_ELA], lr=initial_lr)\n",
    "\n",
    "# Initialize lists to track loss components\n",
    "total_loss_history = []\n",
    "data_fidelity_history = []\n",
    "regularization_history = []\n",
    "total_gradients_history=[]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Update the learning rate\n",
    "    lr = initial_lr - (i / (n_iterations - 1)) * (initial_lr - final_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    \n",
    "    # Perform forward simulation\n",
    "    H_simulated = forward_simulation(Z_ELA, grad_b, b_max,Z_topo,ttot,scaling_factors)\n",
    "\n",
    "    # Compute data missfit\n",
    "    loss = torch.mean((H_simulated - observed_thk)**2)\n",
    "     \n",
    "    # Backpropagate loss and update parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients to the range [-1, 1]\n",
    "    # torch.nn.utils.clip_grad_value_(z_ELA, clip_value=10.0)\n",
    "    # Store the gradients and ela \n",
    "    total_gradients_history.append(Z_ELA.grad.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store loss components for plotting later\n",
    "    total_loss_history.append(loss.item())\n",
    "        # Print loss, gradients and current parameters every 50 iterations \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}, Loss: {loss:.3f} \")\n",
    "        print(f\"Gradient of ELA : {Z_ELA.grad.item()} ELA is {Z_ELA} m\")\n",
    "\n",
    "print_gpu_utilization()  # Print memory after the loop\n",
    "print_peak_gpu_memory()  # Print the peak memory   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m grad_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Mass balance gradient\u001b[39;00m\n\u001b[1;32m      6\u001b[0m b_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Maximum precip (m/yr)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Z_ELA \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3000.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Observed glacier thickness (assumed already loaded as observed_thk tensor)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m observed_thk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObs_2D.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure it's on the right device\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Reset the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Initial guesses for inversion problem\n",
    "grad_b = 0.001  # Mass balance gradient\n",
    "b_max = 0.5  # Maximum precip (m/yr)\n",
    "Z_ELA = torch.tensor(3000.0, requires_grad=True, device=device)\n",
    "\n",
    "# Observed glacier thickness (assumed already loaded as observed_thk tensor)\n",
    "observed_thk = torch.load('Obs_2D.pt', weights_only=True).to(device)  # Ensure it's on the right device\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 80\n",
    "\n",
    "# Optimizer setup: Using L-BFGS\n",
    "optimizer = torch.optim.LBFGS([Z_ELA], lr=1, max_iter=20, history_size=10, line_search_fn='strong_wolfe')\n",
    "\n",
    "# Initialize lists to track loss components\n",
    "total_loss_history = []\n",
    "total_gradients_history = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Define closure function required by LBFGS optimizer\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        # Perform forward simulation\n",
    "        H_simulated = forward_simulation(Z_ELA, grad_b, b_max, Z_topo, ttot, scaling_factors)\n",
    "        # Compute data misfit\n",
    "        loss = torch.mean((H_simulated - observed_thk) ** 2)\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    print_gpu_utilization()  # Print memory after the loop\n",
    "    print_peak_gpu_memory()  # Print the peak memory   \n",
    "\n",
    "    # Perform optimization step\n",
    "    loss = optimizer.step(closure)\n",
    "\n",
    "    # Store the gradients and ELA\n",
    "    total_gradients_history.append(Z_ELA.grad.item())\n",
    "\n",
    "    # Store loss components for plotting later\n",
    "    total_loss_history.append(loss.item())\n",
    "\n",
    "    # Print loss, gradients, and current parameters every 20 iterations\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}, Loss: {loss.item():.3f}\")\n",
    "        print(f\"Gradient of ELA: {Z_ELA.grad.item()}, ELA is {Z_ELA.item()} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do :\n",
    "\n",
    "- Convert all variables to fp16\n",
    "- Check if Gradient accumulation is possible\n",
    "- Read the documentation on memory handling (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# plt.plot(ELA_evolution,label=\"evolution of ELA\",color='b')\n",
    "plt.plot(total_gradients_history, label='Evolution of gradients')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the loss function components\n",
    "def plot_loss_components(total_loss_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the total loss, data fidelity, and regularization term\n",
    "    plt.plot(total_loss_history, label='Loss', color='b', linewidth=2)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Loss Function Components Over Iterations')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the optimization loop\n",
    "plot_loss_components(total_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IGEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
