{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import time, sleep\n",
    "import threading\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 254 MB.\n"
     ]
    }
   ],
   "source": [
    "# Function to track GPU memory usage\n",
    "def track_gpu_utilization(interval=1):\n",
    "    memory_usage = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # Define a helper function to gather GPU memory usage\n",
    "    def record_utilization():\n",
    "        while tracking:\n",
    "            mem = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "            memory_usage.append(mem)\n",
    "            timestamps.append(time() - start_time)\n",
    "            sleep(interval)  # Wait before recording again\n",
    "\n",
    "    # Start tracking in a separate thread\n",
    "    tracking = True\n",
    "    start_time = time()\n",
    "    tracking_thread = threading.Thread(target=record_utilization)\n",
    "    tracking_thread.start()\n",
    "    \n",
    "    return memory_usage, timestamps, lambda: tracking_thread.join()\n",
    "# Visualization function\n",
    "def plot_gpu_utilization(memory_usage, timestamps):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(timestamps, memory_usage, label='GPU Memory Usage (GB)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Memory Usage (GB)')\n",
    "    plt.title('GPU Memory Utilization Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def print_peak_gpu_memory():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
    "    print(f\"Peak GPU memory used: {peak_memory:.2f} MB.\")\n",
    "    torch.cuda.reset_peak_memory_stats()  # Reset after printing\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()  # Initialize NVML\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)  # Assuming we're using GPU 0\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)  # Get memory info\n",
    "    print(f\"GPU memory occupied: {info.used // 1024**2} MB.\")\n",
    "print_gpu_utilization()  # Print memory before starting the optimization loop\n",
    "\n",
    "# Visualization function \n",
    "def visualize_glacier(x, Z, Z_bed, z_ELA, b, Time):\n",
    "    Z_list = Z.cpu().tolist()  # Convert to list\n",
    "    Z_bed_list = Z_bed.cpu().tolist()  # Convert to list\n",
    "    x_list = x.cpu().tolist()  # Convert to list\n",
    "    b_list = b.cpu().tolist()  # Convert to list\n",
    "    clear_output(wait=True)  # Clear the previous output in the notebook\n",
    "    \n",
    "    plt.figure(2, figsize=(7, 5), dpi=200)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_list, 'b', linewidth=2) \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_bed_list, 'k', linewidth=1)\n",
    "    plt.plot([x_list[0] / 1000, x_list[-1] / 1000], [z_ELA, z_ELA], 'g')\n",
    "    plt.ylim([min(Z_bed_list), max(Z_bed_list)])\n",
    "    plt.ylabel('Elevation, m')\n",
    "    plt.title('Glacier après ' + str(int(Time)) + ' années')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot([min(grad_b * (zi - z_ELA), b_max) for zi in Z_list], Z_list, '--k', linewidth=2)\n",
    "    plt.plot([min(b_list), b_max + 0.3], [z_ELA, z_ELA], 'g')\n",
    "    plt.plot([0, 0], [min(Z_bed_list), max(Z_bed_list)], 'k', linewidth=1)\n",
    "    plt.ylim([min(Z_bed_list), max(Z_bed_list)])\n",
    "    plt.title('Fonction bilan de masse') \n",
    "    plt.xlabel('Bilan de masse, m/a')  \n",
    "\n",
    "    plt.subplot(2, 2, 3, aspect=20.0)\n",
    "    plt.plot([xi / 1000 for xi in x_list], b_list, 'b', linewidth=2)\n",
    "    plt.plot([x_list[0] / 1000, x_list[-1] / 1000], [0, 0], 'k', linewidth=1)\n",
    "    plt.title('Bilan de masse effectif')\n",
    "    plt.xlabel('Distance, km') \n",
    "    plt.ylabel('Bilan de masse, m/a') \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Visualization function \n",
    "def visualize_evolution(x, Z_list, Z_bed, z_ELA,observed_thk):\n",
    "    Z_lists=[Z_list[i].cpu().tolist()  for i  in range (len(Z_list))]\n",
    "    Z_bed_list = Z_bed.cpu().tolist()  # Convert to list\n",
    "    x_list = x.cpu().tolist()  # Convert to list\n",
    "    z_ELA_list=z_ELA.cpu().tolist()\n",
    "    Z_observed=(Z_bed+observed_thk).cpu().tolist()\n",
    "\n",
    "    clear_output(wait=False)  # Clear the previous output in the notebook\n",
    "    plt.figure(figsize=(7, 4), dpi=200)\n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_lists[0], 'b', linewidth=1,linestyle='dashed',label='Evolution in time') \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_lists[1], 'b', linewidth=1,linestyle='dashed') \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_lists[2], 'b', linewidth=1,linestyle='dashed') \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_lists[3], 'b', linewidth=1,label='Last thickness') \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_observed, 'r', linewidth=1,label='Observed thickness') \n",
    "    plt.plot([xi / 1000 for xi in x_list], Z_bed_list, 'k', linewidth=1,label='ELA')\n",
    "    plt.plot([x_list[0] / 1000, x_list[-1] / 1000], [z_ELA_list, z_ELA_list], 'g')\n",
    "    plt.ylim([min(Z_bed_list), max(Z_bed_list)])\n",
    "    plt.ylabel('Elevation, m')\n",
    "    plt.title('Glacier evolution')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "def compute_slope(Z, dx):\n",
    "    # Compute the slope (gradient of surface) using finite difference\n",
    "    slope = torch.diff(Z) / dx\n",
    "    return slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare hooks for tensors \n",
    "def print_hook_b(grad):\n",
    "    print(\"\\n db/dELA:\",torch.mean(grad))\n",
    "    return grad\n",
    "def print_hook_h(grad):\n",
    "    print(\"\\n dH/db:\",torch.mean(grad))\n",
    "    return grad\n",
    "def sqrt_hook(grad):\n",
    "    if torch.abs(torch.mean(grad))>1:\n",
    "        print(grad**0.5)\n",
    "        return grad**0.5\n",
    "def reduce_hook(grad):\n",
    "    return grad*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics\n",
    "b_max = 0.3  # maximum ice balance m/yr\n",
    "grad_b = 0.001  # gradient of ice balance as a function of elevation. yr^-1\n",
    "\n",
    "z_ELA = 900  # equilibrium line altitude\n",
    "Lx = 3e5  # model length, m\n",
    "ttot = 1000  # total time\n",
    "rhog = 910 * 9.81  # ice density kg/m^3 * gravity m/s^{-2}\n",
    "fd = 1e-15  # physical constant for ice diffusivity, Pa^3/y\n",
    "kD = fd * rhog**3  # lump all constants for diffusivity together\n",
    "epsilon = 1e-12 # Epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "# Numerics\n",
    "nx = 201  # number of cells\n",
    "dx = Lx / (nx - 1)  # number of cells \n",
    "dtmax = 1      # initial dt, will be changed within loop, yr\n",
    "dt    = dtmax  # initial dt, will be changed within loop, yr\n",
    "x = torch.linspace(0, Lx, nx, device=device)  # x-coordinates\n",
    "nout = 100  # frequency of plotting\n",
    "\n",
    "# Initialization\n",
    "Z_bed = -1000 * torch.log(x + 1000) + 11522.8  # topography\n",
    "H_initial= torch.zeros(nx, device=device) #torch.load('initial_thickness.pt', weights_only=True).to(device)\n",
    "H = H_initial.clone()\n",
    "Time = 0.0  # initialize time\n",
    "nplot = 0  # counter for plotting\n",
    "\n",
    "Z = Z_bed + H  # ice surface\n",
    "\n",
    "it = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is : 2.7312676906585693 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store data for training\n",
    "data = []\n",
    "\n",
    "# Time loop\n",
    "start = time.time()\n",
    "\n",
    "while Time < ttot:\n",
    "\n",
    "    # compute dHdt due to diffusion\n",
    "    H_av = 0.5 * (H[:-1] + H[1:])  # average height between cells (nx-1)\n",
    "    D = kD * H_av**5 * (torch.diff(Z) / dx)**2  # diffusivity (nx-1)\n",
    "    qx = -D * torch.diff(Z) / dx  # ice flux\n",
    "\n",
    "    dHdt = -torch.diff(qx) / dx  # change in ice thickness from flow\n",
    "\n",
    "    # update time step as function of D\n",
    "    max_D = torch.max(D).item()\n",
    "    dt = min(dtmax, dx**2 / (2.1 * (max_D + epsilon)))  # update time step\n",
    "\n",
    "    # update thickness of ice (iceflow)\n",
    "    H[1:-1] += dt * dHdt  # update ice thickness from flow\n",
    "\n",
    "    # update ice thickness (mass balance)\n",
    "    b = torch.minimum(grad_b * (Z - z_ELA), torch.tensor(b_max, device=device)) \n",
    "    H[1:-1] += dt * b[1:-1]\n",
    "\n",
    "    H[H < 0] = 0  # set any negative thickness to 0\n",
    "\n",
    "    # Boundary conditions\n",
    "    H[0] = 0\n",
    "    H[-1] = 0\n",
    "\n",
    "    Z = Z_bed + H  # update ice surface\n",
    "    Time += dt  # update time\n",
    "\n",
    "    it += 1\n",
    "\n",
    "\n",
    "    # Call visualization function\n",
    "    if it % nout == 0:\n",
    " \n",
    "        visualize_glacier(x, Z, Z_bed, z_ELA, b, Time)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time is : {end - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sav e the observations.\n",
    "torch.save(H,'observed_thk.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform forward simulation of glacier thickness with differentiable operations\n",
    "def forward_simulation(H_initial,b_max, grad_b,z_ELA, nx=201, Lx=1e5, ttot=3e3, dtmax=1, device=device,display_time=False):\n",
    "    start= time.time()\n",
    "    # Physics constants\n",
    "    n=3\n",
    "    rhog = 910 * 9.81  # ice density kg/m^3 * gravity m/s^{-2}\n",
    "    fd = 1e-15  # physical constant for ice diffusivity, Pa^3/y\n",
    "    kD = fd * rhog**n  # lump all constants for diffusivity together\n",
    "    Lx = 3e5  # model length, m\n",
    "    dx = Lx / (nx - 1)  # cell size\n",
    "    epsilon = 1e-12\n",
    "\n",
    "    # Numerics\n",
    "    nx = 201  # number of cells\n",
    "    dx = Lx / (nx - 1)  # number of cells \n",
    "    dtmax = 1      # initial dt, will be changed within loop, yr\n",
    "    dt    = dtmax  # initial dt, will be changed within loop, yr\n",
    "    Time = 0\n",
    "\n",
    "    # Initialization\n",
    "    x = torch.linspace(0, Lx, nx, device=device)  # x-coordinates\n",
    "    Z_bed = -1000 * torch.log(x + 1000) + 11522.8  # topography (avoid log(0))\n",
    "    H = H_initial.clone()  # initial ice thickness\n",
    "    Z = Z_bed + H  # ice surface\n",
    "    print(\"one forward run\")\n",
    "    while Time < ttot:\n",
    "        # compute dHdt due to diffusion\n",
    "        H_av = 0.5 * (H[:-1] + H[1:])  # average height between cells (nx-1)\n",
    "        D = kD * H_av**5 * (torch.diff(Z) / dx)**(n-1)  # diffusivity (nx-1)\n",
    "        qx = -D * torch.diff(Z) / dx  # ice flux\n",
    "        # Compute velocity safely\n",
    "        dHdt = -torch.diff(qx) / dx  # change in ice thickness from flow\n",
    "        \n",
    "        # update time step as function of D\n",
    "        max_D = torch.max(D).item()\n",
    "        dt = min(dtmax, dx**2 / (2.1 * (max_D + epsilon)))  # update time step\n",
    "\n",
    "        # update thickness of ice (iceflow)\n",
    "        H[1:-1] += dt * dHdt  # update ice thickness from flow\n",
    "    \n",
    "        # update ice thickness (mass balance)\n",
    "        b = torch.minimum(grad_b * (Z - z_ELA), torch.tensor(b_max, device=device)) \n",
    "        b.retain_grad() \n",
    "        b.register_hook(reduce_hook)\n",
    "        \n",
    "        H[1:-1] += dt * b[1:-1]\n",
    "        H.retain_grad()\n",
    "        H.register_hook(reduce_hook)\n",
    "        H[H < 0] = 0  # set any negative thickness to 0\n",
    "    \n",
    "        # Boundary conditions\n",
    "        H[0] = 0\n",
    "        H[-1] = 0\n",
    "    \n",
    "        Z = Z_bed + H  # update ice surface\n",
    "        Time += dt  # update time\n",
    "       \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  1 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  2 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  3 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  4 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  5 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  6 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  7 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  8 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  9 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  10 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  11 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  12 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  13 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  14 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  15 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  16 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  17 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  18 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  19 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "\n",
      "Iteration 20/80, Loss: 1620.950 \n",
      "Gradient of ELA : 0.00027421346749179065 ELA is 1189.4554443359375 m\n",
      "Iteration  20 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  21 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  22 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  23 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  24 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  25 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  26 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  27 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  28 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0004, device='cuda:0')\n",
      "Iteration  29 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  30 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  31 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  32 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  33 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  34 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  35 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  36 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  37 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  38 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  39 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "\n",
      "Iteration 40/80, Loss: 1184.560 \n",
      "Gradient of ELA : 0.00031924861832521856 ELA is 1174.7962646484375 m\n",
      "Iteration  40 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  41 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  42 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  43 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  44 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  45 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  46 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  47 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  48 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  49 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  50 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0002, device='cuda:0')\n",
      "Iteration  51 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  52 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0015, device='cuda:0')\n",
      "Iteration  53 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0029, device='cuda:0')\n",
      "Iteration  54 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0019, device='cuda:0')\n",
      "Iteration  55 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0009, device='cuda:0')\n",
      "Iteration  56 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0014, device='cuda:0')\n",
      "Iteration  57 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0006, device='cuda:0')\n",
      "Iteration  58 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0003, device='cuda:0')\n",
      "Iteration  59 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0020, device='cuda:0')\n",
      "\n",
      "Iteration 60/80, Loss: 501.811 \n",
      "Gradient of ELA : 0.0020087999291718006 ELA is 1155.248779296875 m\n",
      "Iteration  60 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0006, device='cuda:0')\n",
      "Iteration  61 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(2.3773e-05, device='cuda:0')\n",
      "Iteration  62 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0010, device='cuda:0')\n",
      "Iteration  63 --------------------------------------------------\n",
      "one forward run\n",
      "\n",
      " dH/db: tensor(0.0007, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m data_fidelity \u001b[38;5;241m+\u001b[39m reg_lambda \u001b[38;5;241m*\u001b[39m smoothness_reg\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Backpropagate loss and update parameters\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m total_gradients_history\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mmean(z_ELA\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IGEM/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initial guesses for inversion problem\n",
    "b_max = 0.3\n",
    "grad_b = 0.001\n",
    "z_ELA = torch.full((nx,),1200., requires_grad=True, device=device)\n",
    "# Observed glacier thickness (assumed already loaded as observed_thk tensor)\n",
    "observed_thk = torch.load('observed_thk.pt', weights_only=True).to(device) # Ensure it's on the right device\n",
    "\n",
    "# Define initial and final learning rates\n",
    "initial_lr = 7\n",
    "final_lr = 7\n",
    "n_iterations = 80\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam([z_ELA], lr=initial_lr)\n",
    "reg_lambda=0.001\n",
    "\n",
    "# Initialize lists to track loss components\n",
    "total_loss_history = []\n",
    "data_fidelity_history = []\n",
    "regularization_history = []\n",
    "total_gradients_history=[]\n",
    "ELA_evolution=[]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Update the learning rate\n",
    "    lr = initial_lr - (i / (n_iterations - 1)) * (initial_lr - final_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    print(\"Iteration \", i,50*'-')\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    \n",
    "    # Perform forward simulation\n",
    "    H_simulated = forward_simulation(H_initial,b_max, grad_b, z_ELA,ttot=ttot,display_time=False)\n",
    "    h=z_ELA.register_hook(print_hook_h)\n",
    "    \n",
    "    # Compute data fidelity and regularization\n",
    "    data_fidelity = torch.mean((H_simulated - observed_thk) ** 2)\n",
    "    smoothness_reg = torch.sum((z_ELA[1:] - z_ELA[:-1]) ** 2)\n",
    "    \n",
    "    # Compute total loss\n",
    "    loss = data_fidelity + reg_lambda * smoothness_reg\n",
    "    # Backpropagate loss and update parameters\n",
    "    loss.backward()\n",
    "\n",
    "    total_gradients_history.append(torch.mean(z_ELA.grad).cpu())\n",
    "\n",
    "    optimizer.step()\n",
    "    h.remove()\n",
    "\n",
    "    # Store loss components for plotting later\n",
    "    total_loss_history.append(loss.item())\n",
    "    data_fidelity_history.append(data_fidelity.item())\n",
    "    regularization_history.append((reg_lambda * smoothness_reg).item())\n",
    "\n",
    "    # Print loss, gradients and current parameters every 50 iterations \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}, Loss: {loss:.3f} \")\n",
    "        print(f\"Gradient of ELA : {torch.mean(z_ELA.grad)} ELA is {torch.mean(z_ELA)} m\")\n",
    "\n",
    "print_gpu_utilization()  # Print memory after the loop\n",
    "print_peak_gpu_memory()  # Print the peak memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# plt.plot(ELA_evolution,label=\"evolution of ELA\",color='b')\n",
    "plt.plot(total_gradients_history, label='Evolution of gradients')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77633/1814538576.py:11: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z_bed_list = Z_bed.cpu().tolist()  # Convert to list\n",
    "x_list = x.cpu().tolist()  # Convert to list\n",
    "z_ELA_list=z_ELA.cpu().tolist()\n",
    "clear_output(wait=False)  # Clear the previous output in the notebook\n",
    "plt.figure(figsize=(7, 4), dpi=200)\n",
    "\n",
    "plt.plot([xi / 1000 for xi in x_list],z_ELA_list, 'g')\n",
    "# plt.ylim([min(Z_bed_list), max(Z_bed_list)])\n",
    "plt.ylabel('Elevation, m')\n",
    "plt.title('Spatial ELA ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the loss function components\n",
    "def plot_loss_components(total_loss_history, data_fidelity_history, regularization_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the total loss, data fidelity, and regularization term\n",
    "    plt.plot(total_loss_history, label='Total Loss', color='b', linewidth=2)\n",
    "    plt.plot(data_fidelity_history, label='Data Fidelity', color='g', linestyle='--', linewidth=2)\n",
    "    plt.plot(regularization_history, label='Regularization (Smoothness)', color='r', linestyle='-.', linewidth=2)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Loss Function Components Over Iterations')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After the optimization loop\n",
    "plot_loss_components(total_loss_history, data_fidelity_history, regularization_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IGEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
